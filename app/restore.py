import os
import subprocess
import gzip
import asyncio
from datetime import datetime
from typing import Optional
import psycopg2
from .models import BackupInfo, DatabaseConfig, BackupConfig, RestoreResponse
from .backup import BackupManager


class RestoreManager:
    def __init__(self, db_config: DatabaseConfig, backup_config: BackupConfig):
        self.db_config = db_config
        self.backup_config = backup_config
        self.backup_manager = BackupManager(db_config, backup_config)
    
    async def restore_backup(self, backup_id: str, restore_type: str = "full", force: bool = False) -> RestoreResponse:
        """æ¢å¤æŒ‡å®šçš„å¤‡ä»½"""
        backup_info = self.backup_manager.load_backup_info(backup_id)
        if not backup_info:
            raise ValueError(f"å¤‡ä»½ {backup_id} ä¸å­˜åœ¨")
        
        if backup_info.status != "completed":
            raise ValueError(f"å¤‡ä»½ {backup_id} çŠ¶æ€ä¸æ­£ç¡®: {backup_info.status}")
        
        backup_file = os.path.join(
            self.backup_config.storage_path,
            backup_info.filename
        )
        
        if not os.path.exists(backup_file):
            raise ValueError(f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_file}")
        
        try:
            # æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§
            if not force:
                await self.check_version_compatibility(backup_info)
            
            # æ ¹æ®æ¢å¤ç±»å‹æ‰§è¡Œä¸åŒçš„æ¢å¤ç­–ç•¥
            if restore_type == "normal":
                # æ™®é€šæ¢å¤ - ä½¿ç”¨åŸæ¥çš„æ¢å¤é€»è¾‘
                await self.execute_restore(backup_file, backup_info.compressed)
                message = f"æ¢å¤å¤‡ä»½ {backup_id} æˆåŠŸ"
            elif restore_type == "full":
                await self.execute_full_restore(backup_file, backup_info.compressed)
                message = f"å®Œå…¨æ¢å¤å¤‡ä»½ {backup_id} æˆåŠŸ"
            elif restore_type == "incremental":
                await self.execute_incremental_restore(backup_file, backup_info.compressed)
                message = f"å¢é‡æ¢å¤å¤‡ä»½ {backup_id} æˆåŠŸ"
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¢å¤ç±»å‹: {restore_type}")
            
            return RestoreResponse(
                success=True,
                message=message,
                backup_id=backup_id,
                restored_at=datetime.now()
            )
            
        except Exception as e:
            return RestoreResponse(
                success=False,
                message=f"æ¢å¤å¤±è´¥: {str(e)}",
                backup_id=backup_id,
                restored_at=datetime.now()
            )
    
    async def check_version_compatibility(self, backup_info: BackupInfo):
        """æ£€æŸ¥Alembicç‰ˆæœ¬å…¼å®¹æ€§"""
        if backup_info.alembic_version:
            current_version = self.backup_manager.get_alembic_version()
            if current_version and current_version != backup_info.alembic_version:
                print(f"è­¦å‘Š: å½“å‰ç‰ˆæœ¬ {current_version} ä¸å¤‡ä»½ç‰ˆæœ¬ {backup_info.alembic_version} ä¸åŒ¹é…")
                # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´ä¸¥æ ¼çš„ç‰ˆæœ¬æ£€æŸ¥é€»è¾‘
    
    async def execute_full_restore(self, backup_file: str, compressed: bool):
        """æ‰§è¡Œå®Œå…¨æ¢å¤ - å…ˆæ¸…ç©ºæ•°æ®åº“ï¼Œå†æ¢å¤"""
        print("æ‰§è¡Œå®Œå…¨æ¢å¤...")
        
        # å…ˆæ¸…ç©ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨
        await self.clear_database()
        
        # ç„¶åæ‰§è¡Œæ ‡å‡†æ¢å¤
        await self.execute_restore(backup_file, compressed)
    
    async def execute_incremental_restore(self, backup_file: str, compressed: bool):
        """ç”¨å¯é é€»è¾‘å®ç°å¢é‡æ¢å¤ï¼šåªè¡¥é½ç¼ºå¤±æ•°æ®"""
        import re
        print("ğŸ”„ [æ–°] æ‰§è¡Œç®€å•å¢é‡æ¢å¤...")
        # 1. è¯»å–å¤‡ä»½æ–‡ä»¶å†…å®¹
        if compressed:
            import gzip
            with gzip.open(backup_file, 'rt', encoding='utf-8') as f:
                content = f.read()
        else:
            with open(backup_file, 'r', encoding='utf-8') as f:
                content = f.read()
        # 2. è§£ææ‰€æœ‰è¡¨çš„COPYæ•°æ®
        copy_pattern = r"COPY public\.(\w+)\s*\(([^)]+)\)\s*FROM stdin;\n(.*?)\n\\\."
        tables = {}
        for match in re.finditer(copy_pattern, content, re.DOTALL):
            table_name = match.group(1)
            columns_str = match.group(2)
            data_content = match.group(3)
            columns = [col.strip().strip('"') for col in columns_str.split(',')]
            rows = []
            for line in data_content.strip().split('\n'):
                if line.strip():
                    row_data = line.strip().split('\t')
                    if len(row_data) == len(columns):
                        rows.append(row_data)
            tables[table_name] = {
                'columns': columns,
                'rows': rows,
                'count': len(rows)
            }
            print(f"   ğŸ“‹ è¡¨ {table_name}: {len(rows)} è¡Œ")
            print(f"   ğŸ“Š åˆ—: {columns}")
        # 3. å¯¹æ¯”å¹¶è¡¥é½æ¯ä¸ªè¡¨
        import psycopg2
        total_inserted = 0
        for table_name, backup_data in tables.items():
            print(f"\nğŸ“Š å¤„ç†è¡¨: {table_name}")
            # è·å–å½“å‰æ•°æ®åº“æ•°æ®
            conn = psycopg2.connect(
                host=self.db_config.host,
                port=self.db_config.port,
                database=self.db_config.database,
                user=self.db_config.username,
                password=self.db_config.password
            )
            cursor = conn.cursor()
            cursor.execute(f"""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name = %s AND table_schema = 'public' 
                ORDER BY ordinal_position
            """, (table_name,))
            db_columns = [row[0] for row in cursor.fetchall()]
            cursor.execute(f'SELECT * FROM "{table_name}" ORDER BY 1')
            db_rows = cursor.fetchall()
            # è·å–ä¸»é”®
            cursor.execute('''
                SELECT a.attname
                FROM pg_index i
                JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)
                WHERE i.indrelid = %s::regclass AND i.indisprimary;
            ''', (table_name,))
            pk_columns = [row[0] for row in cursor.fetchall()]
            print(f"   ä¸»é”®å­—æ®µ: {pk_columns}")
            if not pk_columns:
                print(f"   âš ï¸ è¡¨ {table_name} æ²¡æœ‰ä¸»é”®ï¼Œè·³è¿‡")
                conn.close()
                continue
            # å½“å‰æ•°æ®çš„ä¸»é”®é›†åˆ
            def create_key(row, columns, pk_columns):
                return '|'.join([f"{pk}:{str(row[columns.index(pk)])}" for pk in pk_columns if pk in columns])
            db_keys = set()
            for row in db_rows:
                db_keys.add(create_key(row, db_columns, pk_columns))
            # æ‰¾å‡ºç¼ºå¤±çš„è¡Œ
            missing_rows = []
            for row in backup_data['rows']:
                key = create_key(row, backup_data['columns'], pk_columns)
                if key not in db_keys:
                    missing_rows.append(row)
            print(f"   ç¼ºå¤±è¡Œæ•°: {len(missing_rows)}")
            if missing_rows:
                print(f"   ç¼ºå¤±çš„ä¸»é”® (å‰5ä¸ª):")
                for i, row in enumerate(missing_rows[:5]):
                    key = create_key(row, backup_data['columns'], pk_columns)
                    print(f"     {i+1}. {key}")
                # ç±»å‹è½¬æ¢
                cursor.execute(f"""
                    SELECT column_name, data_type
                    FROM information_schema.columns 
                    WHERE table_name = %s AND table_schema = 'public' 
                    ORDER BY ordinal_position
                """, (table_name,))
                column_info = cursor.fetchall()
                column_types = {col[0]: col[1] for col in column_info}
                converted_rows = []
                for row in missing_rows:
                    converted_row = []
                    for i, (col_name, value) in enumerate(zip(backup_data['columns'], row)):
                        try:
                            dtype = column_types.get(col_name, 'text')
                            if value is None or value == '' or value == 'NULL':
                                converted_row.append(None)
                            elif dtype.startswith('int'):
                                converted_row.append(int(value))
                            elif dtype.startswith('numeric') or dtype.startswith('float'):
                                converted_row.append(float(value))
                            elif dtype.startswith('date'):
                                from datetime import datetime
                                converted_row.append(datetime.strptime(value, '%Y-%m-%d').date())
                            else:
                                converted_row.append(str(value))
                        except Exception as e:
                            print(f"âš ï¸ ç±»å‹è½¬æ¢å¤±è´¥: {col_name}={value} -> {e}")
                            converted_row.append(value)
                    converted_rows.append(converted_row)
                # æ’å…¥
                placeholders = ', '.join(['%s'] * len(backup_data['columns']))
                column_names = ', '.join([f'"{col}"' for col in backup_data['columns']])
                insert_sql = f'INSERT INTO "{table_name}" ({column_names}) VALUES ({placeholders}) ON CONFLICT DO NOTHING'
                try:
                    cursor.executemany(insert_sql, converted_rows)
                    conn.commit()
                    inserted_count = cursor.rowcount
                    print(f"âœ… è¡¨ {table_name} æˆåŠŸæ’å…¥ {inserted_count} è¡Œ")
                    total_inserted += inserted_count
                except Exception as e:
                    print(f"âŒ æ’å…¥å¤±è´¥: {e}")
                    conn.rollback()
            else:
                print(f"   âœ… è¡¨ {table_name} æ— éœ€æ¢å¤")
            conn.close()
        print(f"\n" + "=" * 60)
        print(f"ğŸ“‹ å¢é‡æ¢å¤å®Œæˆ")
        print(f"âœ… æ€»å…±æ’å…¥ {total_inserted} è¡Œæ•°æ®")
    
    async def get_current_database_snapshot(self) -> dict:
        """è·å–å½“å‰æ•°æ®åº“çš„æ•°æ®å¿«ç…§"""
        try:
            conn = psycopg2.connect(
                host=self.db_config.host,
                port=self.db_config.port,
                database=self.db_config.database,
                user=self.db_config.username,
                password=self.db_config.password
            )
            cursor = conn.cursor()
            
            snapshot = {}
            
            # è·å–æ‰€æœ‰è¡¨å
            cursor.execute("""
                SELECT tablename FROM pg_tables 
                WHERE schemaname = 'public'
            """)
            tables = cursor.fetchall()
            
            # è·å–æ¯ä¸ªè¡¨çš„æ•°æ®
            for table in tables:
                table_name = table[0]
                try:
                    cursor.execute(f"SELECT * FROM {table_name}")
                    rows = cursor.fetchall()
                    
                    # è·å–åˆ—å
                    cursor.execute(f"""
                        SELECT column_name, data_type 
                        FROM information_schema.columns 
                        WHERE table_name = %s AND table_schema = 'public' 
                        ORDER BY ordinal_position
                    """, (table_name,))
                    columns_info = cursor.fetchall()
                    
                    snapshot[table_name] = {
                        'columns': [col[0] for col in columns_info],
                        'types': [col[1] for col in columns_info],
                        'rows': rows
                    }
                except Exception as e:
                    print(f"è·å–è¡¨ {table_name} æ•°æ®å¤±è´¥: {e}")
                    continue
            
            conn.close()
            return snapshot
            
        except Exception as e:
            print(f"è·å–æ•°æ®åº“å¿«ç…§å¤±è´¥: {e}")
            return {}
    
    def parse_backup_data(self, sql_content: str) -> dict:
        """è§£æå¤‡ä»½æ–‡ä»¶ä¸­çš„æ•°æ®"""
        backup_data = {}
        current_table = None
        current_columns = []
        current_rows = []
        in_copy_section = False
        
        lines = sql_content.split('\n')
        
        for line in lines:
            line_upper = line.upper().strip()
            
            # æ£€æµ‹CREATE TABLEè¯­å¥
            if line_upper.startswith('CREATE TABLE'):
                # ä¿å­˜å‰ä¸€ä¸ªè¡¨çš„æ•°æ®
                if current_table and current_columns:
                    backup_data[current_table] = {
                        'columns': current_columns,
                        'rows': current_rows,
                        'count': len(current_rows)
                    }
                    print(f"[è§£æå¤‡ä»½] è¡¨ {current_table} è§£æå®Œæˆï¼Œå…± {len(current_rows)} è¡Œæ•°æ®")
                
                # å¼€å§‹æ–°è¡¨ - å»æ‰schemaå‰ç¼€
                table_name = line.split()[2].strip('"')
                if '.' in table_name:
                    table_name = table_name.split('.')[-1]  # å»æ‰schemaå‰ç¼€
                current_table = table_name
                current_columns = []
                current_rows = []
                in_copy_section = False
                print(f"[è§£æå¤‡ä»½] å‘ç°è¡¨: {table_name}")
            
            # æ£€æµ‹COPYè¯­å¥
            elif line_upper.startswith('COPY ') and current_table:
                in_copy_section = True
                # è§£æåˆ—å
                if '(' in line and ')' in line:
                    cols_part = line[line.find('(')+1:line.find(')')]
                    current_columns = [col.strip().strip('"') for col in cols_part.split(',')]
                    print(f"[è§£æå¤‡ä»½] è¡¨ {current_table} çš„åˆ—: {current_columns}")
            
            # æ£€æµ‹æ•°æ®è¡Œ
            elif in_copy_section and line.strip() and not line.startswith('\\'):
                if line.strip() == r'\.':
                    in_copy_section = False
                else:
                    # è§£ææ•°æ®è¡Œ
                    row_data = line.strip().split('\t')
                    if len(row_data) == len(current_columns):
                        # è½¬æ¢æ•°æ®ç±»å‹
                        converted_row = self.convert_backup_row_data(row_data)
                        current_rows.append(converted_row)
        
        # ä¿å­˜æœ€åä¸€ä¸ªè¡¨
        if current_table and current_columns:
            backup_data[current_table] = {
                'columns': current_columns,
                'rows': current_rows,
                'count': len(current_rows)
            }
            print(f"[è§£æå¤‡ä»½] è¡¨ {current_table} è§£æå®Œæˆï¼Œå…± {len(current_rows)} è¡Œæ•°æ®")
        
        return backup_data
    
    def convert_backup_row_data(self, row_data: list) -> list:
        """è½¬æ¢å¤‡ä»½è¡Œæ•°æ®çš„æ•°æ®ç±»å‹"""
        converted = []
        for val in row_data:
            if val == '\\N' or val == '':
                converted.append(None)
            elif val.lower() == 'true':
                converted.append(True)
            elif val.lower() == 'false':
                converted.append(False)
            else:
                # å°è¯•è½¬æ¢ä¸ºæ•°å­—
                try:
                    if '.' in val:
                        converted.append(float(val))
                    else:
                        converted.append(int(val))
                except ValueError:
                    converted.append(val)
        return converted
    
    def get_primary_key_columns(self, table_name: str) -> list:
        """è·å–è¡¨çš„ä¸»é”®å­—æ®µååˆ—è¡¨"""
        try:
            conn = psycopg2.connect(
                host=self.db_config.host,
                port=self.db_config.port,
                database=self.db_config.database,
                user=self.db_config.username,
                password=self.db_config.password
            )
            cursor = conn.cursor()
            cursor.execute('''
                SELECT a.attname
                FROM pg_index i
                JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)
                WHERE i.indrelid = %s::regclass AND i.indisprimary;
            ''', (table_name,))
            pk_columns = [row[0] for row in cursor.fetchall()]
            conn.close()
            return pk_columns
        except Exception as e:
            print(f"è·å–è¡¨ {table_name} ä¸»é”®å­—æ®µå¤±è´¥: {e}")
            return []

    def create_row_key(self, row: list, columns: list, pk_columns: Optional[list] = None) -> str:
        if pk_columns is None:
            pk_columns = []
        # ä¸»é”®å­—æ®µå¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œä¿è¯ç±»å‹ä¸€è‡´
        if pk_columns:
            key_parts = []
            for pk in pk_columns:
                if pk in columns:
                    idx = columns.index(pk)
                    val = row[idx]
                    key_parts.append(f"{pk}:{str(val) if val is not None else 'NULL'}")
            return '|'.join(key_parts)
        # fallback: å…¨å­—æ®µ
        key_parts = []
        for i, val in enumerate(row):
            key_parts.append(f"{columns[i]}:{str(val) if val is not None else 'NULL'}")
        return '|'.join(key_parts)

    def create_row_keys(self, rows: list, columns: list, pk_columns: Optional[list] = None) -> set:
        """ä¸ºæ‰€æœ‰è¡Œåˆ›å»ºå”¯ä¸€æ ‡è¯†é›†åˆï¼Œä¼˜å…ˆç”¨ä¸»é”®å­—æ®µ"""
        if pk_columns is None:
            pk_columns = []
        keys = set()
        for row in rows:
            key = self.create_row_key(row, columns, pk_columns)
            keys.add(key)
        return keys

    def calculate_incremental_restore_data(self, current_snapshot: dict, backup_data: dict) -> list:
        """åªè¡¥é½å¤‡ä»½æœ‰ä½†å½“å‰æ²¡æœ‰çš„æ•°æ®ï¼Œå”¯ä¸€æ€§ä¼˜å…ˆç”¨ä¸»é”®"""
        restore_data = []
        for table_name, backup_table_data in backup_data.items():
            if table_name not in current_snapshot:
                # è¡¨ä¸å­˜åœ¨ï¼Œè·³è¿‡ï¼ˆæˆ–å¯é€‰å®ç°åˆ›å»ºè¡¨ï¼‰
                print(f"[å¢é‡æ¢å¤] è¡¨ {table_name} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                continue
            current_table_data = current_snapshot[table_name]
            pk_columns = self.get_primary_key_columns(table_name) or []
            # ç”Ÿæˆå”¯ä¸€keyé›†åˆ
            current_keys = self.create_row_keys(current_table_data['rows'], current_table_data['columns'], pk_columns)
            backup_keys = self.create_row_keys(backup_table_data['rows'], backup_table_data['columns'], pk_columns)
            # æ‰¾å‡ºå¤‡ä»½æœ‰ä½†å½“å‰æ²¡æœ‰çš„key
            missing_keys = backup_keys - current_keys
            missing_rows = []
            for row in backup_table_data['rows']:
                row_key = self.create_row_key(row, backup_table_data['columns'], pk_columns)
                if row_key in missing_keys:
                    missing_rows.append(row)
            if missing_rows:
                print(f"[å¢é‡æ¢å¤] è¡¨ {table_name} éœ€è¡¥é½ {len(missing_rows)} è¡Œ")
                restore_data.append({
                    'type': 'rows_missing',
                    'table': table_name,
                    'missing_rows': missing_rows,
                    'columns': backup_table_data['columns']
                })
        return restore_data
    
    async def execute_incremental_restore_data(self, restore_data: list):
        """æ‰§è¡Œå¢é‡æ¢å¤æ•°æ®"""
        for item in restore_data:
            if item['type'] == 'table_missing':
                # è¡¨ä¸å­˜åœ¨ï¼Œéœ€è¦åˆ›å»ºè¡¨å¹¶æ’å…¥æ‰€æœ‰æ•°æ®
                print(f"è¡¨ {item['table']} ä¸å­˜åœ¨ï¼Œéœ€è¦åˆ›å»ºè¡¨ç»“æ„")
                await self.create_table_and_restore_data(item)
                
            elif item['type'] == 'rows_missing':
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                if await self.table_exists(item['table']):
                    # ä¸ºæ¯ä¸ªè¡¨å•ç‹¬å¤„ç†ï¼Œé¿å…äº‹åŠ¡å†²çª
                    await self.restore_table_missing_rows(item)
                else:
                    print(f"è¡¨ {item['table']} ä¸å­˜åœ¨ï¼Œè·³è¿‡æ•°æ®æ¢å¤")
    
    async def create_table_and_restore_data(self, item: dict):
        """åˆ›å»ºè¡¨å¹¶æ¢å¤æ•°æ®"""
        table_name = item['table']
        table_data = item['data']
        
        print(f"å°è¯•åˆ›å»ºè¡¨ {table_name} å¹¶æ¢å¤æ•°æ®")
        
        # è¿™é‡Œéœ€è¦ä»å¤‡ä»½æ–‡ä»¶ä¸­æå–CREATE TABLEè¯­å¥
        # ç”±äºå½“å‰å®ç°é™åˆ¶ï¼Œæˆ‘ä»¬æš‚æ—¶è·³è¿‡è¡¨åˆ›å»º
        print(f"è¡¨ {table_name} åˆ›å»ºåŠŸèƒ½æš‚æœªå®ç°ï¼Œè·³è¿‡")
        print(f"å»ºè®®å…ˆä½¿ç”¨æ™®é€šæ¢å¤æˆ–å®Œå…¨æ¢å¤æ¥åˆ›å»ºè¡¨ç»“æ„")
    
    async def table_exists(self, table_name: str) -> bool:
        """æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨"""
        try:
            conn = psycopg2.connect(
                host=self.db_config.host,
                port=self.db_config.port,
                database=self.db_config.database,
                user=self.db_config.username,
                password=self.db_config.password
            )
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = %s
                );
            """, (table_name,))
            
            result = cursor.fetchone()
            exists = result[0] if result else False
            
            conn.close()
            return exists
            
        except Exception as e:
            print(f"æ£€æŸ¥è¡¨ {table_name} æ˜¯å¦å­˜åœ¨æ—¶å‡ºé”™: {e}")
            return False
    
    async def restore_table_missing_rows(self, item: dict):
        """æ¢å¤å•ä¸ªè¡¨çš„ç¼ºå¤±è¡Œ - æ”¹è¿›ç‰ˆæœ¬"""
        table_name = item['table']
        columns = item['columns']
        missing_rows = item['missing_rows']
        print(f"ğŸ”„ æ¢å¤è¡¨ {table_name} ä¸­ç¼ºå¤±çš„ {len(missing_rows)} è¡Œæ•°æ®")
        if not missing_rows:
            print(f"âœ… è¡¨ {table_name} æ— éœ€æ¢å¤æ•°æ®")
            return
        # è·å–ä¸»é”®å­—æ®µ
        pk_columns = self.get_primary_key_columns(table_name)
        # å°è¯•æ‰¹é‡æ’å…¥ä»¥æé«˜æ€§èƒ½
        batch_size = 100
        success_count = 0
        error_count = 0
        for i in range(0, len(missing_rows), batch_size):
            batch = missing_rows[i:i + batch_size]
            try:
                # æè¯¦ç»†æ—¥å¿—ï¼šæ‰“å°æ¯ä¸€æ¡ç¼ºå¤±è¡Œçš„ä¸»é”®å’Œå€¼
                for row in batch:
                    pk_info = ', '.join([f'{pk}={row[columns.index(pk)]}' for pk in pk_columns if pk in columns])
                    print(f"[å¢é‡æ¢å¤][å‡†å¤‡æ’å…¥] ä¸»é”®: {pk_info} å…¨éƒ¨å­—æ®µ: {dict(zip(columns, row))}")
                batch_success = await self.insert_batch_rows(table_name, columns, batch)
                success_count += batch_success
                print(f"ğŸ“¦ æ‰¹æ¬¡ {i//batch_size + 1}: æˆåŠŸæ’å…¥ {batch_success}/{len(batch)} è¡Œ")
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡ {i//batch_size + 1} å¤±è´¥: {e}")
                error_count += len(batch)
                # å¦‚æœæ‰¹é‡æ’å…¥å¤±è´¥ï¼Œå°è¯•é€è¡Œæ’å…¥
                for row in batch:
                    try:
                        pk_info = ', '.join([f'{pk}={row[columns.index(pk)]}' for pk in pk_columns if pk in columns])
                        print(f"[å¢é‡æ¢å¤][å•è¡Œæ’å…¥] ä¸»é”®: {pk_info} å…¨éƒ¨å­—æ®µ: {dict(zip(columns, row))}")
                        await self.insert_single_row(table_name, columns, row)
                        success_count += 1
                    except Exception as row_error:
                        print(f"âŒ å•è¡Œæ’å…¥å¤±è´¥: {row_error}")
                        error_count += 1
        print(f"âœ… è¡¨ {table_name} æ¢å¤å®Œæˆ: æˆåŠŸ {success_count} è¡Œ, å¤±è´¥ {error_count} è¡Œ")
    
    async def insert_batch_rows(self, table_name: str, columns: list, rows: list) -> int:
        if not rows:
            return 0
        conn = None
        try:
            conn = psycopg2.connect(
                host=self.db_config.host,
                port=self.db_config.port,
                database=self.db_config.database,
                user=self.db_config.username,
                password=self.db_config.password
            )
            cursor = conn.cursor()
            cursor.execute(f"""
                SELECT column_name, data_type, is_nullable
                FROM information_schema.columns 
                WHERE table_name = %s AND table_schema = 'public' 
                ORDER BY ordinal_position
            """, (table_name,))
            column_info = cursor.fetchall()
            column_types = {col[0]: col[1] for col in column_info}
            column_nullable = {col[0]: col[2] == 'YES' for col in column_info}
            # è½¬æ¢æ‰€æœ‰è¡Œçš„æ•°æ®ç±»å‹
            converted_rows = []
            for row in rows:
                converted_row = []
                for i, (col_name, value) in enumerate(zip(columns, row)):
                    try:
                        # ç±»å‹è½¬æ¢ï¼šint/float/str/date
                        dtype = column_types.get(col_name, 'text')
                        if value is None or value == '' or value == 'NULL':
                            converted_row.append(None)
                        elif dtype.startswith('int'):
                            converted_row.append(int(value))
                        elif dtype.startswith('numeric') or dtype.startswith('float') or dtype.startswith('double'):
                            converted_row.append(float(value))
                        elif dtype.startswith('date'):
                            from datetime import datetime
                            converted_row.append(datetime.strptime(value, '%Y-%m-%d').date())
                        else:
                            converted_row.append(str(value))
                    except Exception as e:
                        print(f"âš ï¸ æ•°æ®ç±»å‹è½¬æ¢å¤±è´¥: {col_name}={value} -> {e}")
                        converted_row.append(value)
                converted_rows.append(converted_row)
            placeholders = ', '.join(['%s'] * len(columns))
            column_names = ', '.join([f'"{col}"' for col in columns])
            insert_sql = f'INSERT INTO "{table_name}" ({column_names}) VALUES ({placeholders})'
            if self.has_primary_key(table_name):
                insert_sql += ' ON CONFLICT DO NOTHING'
            # æ—¥å¿—ï¼šæ‰“å°å°†è¦æ’å…¥çš„ä¸»é”®
            pk_columns = self.get_primary_key_columns(table_name)
            for row in converted_rows:
                pk_info = ', '.join([f'{pk}={row[columns.index(pk)]}' for pk in pk_columns if pk in columns])
                print(f"[å¢é‡æ¢å¤] æ’å…¥å‰ä¸»é”®ä¿¡æ¯: {pk_info}")
            cursor.executemany(insert_sql, converted_rows)
            conn.commit()
            print(f"[å¢é‡æ¢å¤] è¡¨ {table_name} æˆåŠŸæ’å…¥ {len(converted_rows)} è¡Œæ•°æ®")
            return len(converted_rows)
        except Exception as e:
            print(f"âŒ æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
            if conn:
                conn.rollback()
            return 0
        finally:
            if conn:
                conn.close()
    
    async def insert_single_row(self, table_name: str, columns: list, row_data: list):
        """æ’å…¥å•è¡Œæ•°æ® - æ”¹è¿›ç‰ˆæœ¬ï¼Œå¤„ç†æ•°æ®ç±»å‹è½¬æ¢"""
        conn = None
        try:
            conn = psycopg2.connect(
                host=self.db_config.host,
                port=self.db_config.port,
                database=self.db_config.database,
                user=self.db_config.username,
                password=self.db_config.password
            )
            cursor = conn.cursor()
            
            # è·å–è¡¨çš„åˆ—ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ•°æ®ç±»å‹
            cursor.execute(f"""
                SELECT column_name, data_type, is_nullable
                FROM information_schema.columns 
                WHERE table_name = %s AND table_schema = 'public' 
                ORDER BY ordinal_position
            """, (table_name,))
            column_info = cursor.fetchall()
            
            # åˆ›å»ºåˆ—ååˆ°æ•°æ®ç±»å‹çš„æ˜ å°„
            column_types = {col[0]: col[1] for col in column_info}
            column_nullable = {col[0]: col[2] == 'YES' for col in column_info}
            
            # è½¬æ¢æ•°æ®ç±»å‹
            converted_data = []
            for i, (col_name, value) in enumerate(zip(columns, row_data)):
                try:
                    converted_value = self.convert_value_for_column(value, column_types.get(col_name, 'text'), column_nullable.get(col_name, True))
                    converted_data.append(converted_value)
                except Exception as e:
                    print(f"âš ï¸ åˆ— {col_name} æ•°æ®ç±»å‹è½¬æ¢å¤±è´¥: {value} -> {e}")
                    # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å€¼
                    converted_data.append(value)
            
            placeholders = ', '.join(['%s'] * len(columns))
            column_names = ', '.join([f'"{col}"' for col in columns])
            
            insert_sql = f'INSERT INTO "{table_name}" ({column_names}) VALUES ({placeholders})'
            
            # ä½¿ç”¨ON CONFLICT DO NOTHINGé¿å…ä¸»é”®å†²çª
            if self.has_primary_key(table_name):
                insert_sql += ' ON CONFLICT DO NOTHING'
            
            cursor.execute(insert_sql, converted_data)
            
            conn.commit()
            
        except psycopg2.IntegrityError as e:
            if conn:
                conn.rollback()
            print(f"âš ï¸ æ•°æ®å®Œæ•´æ€§é”™è¯¯ (è¡¨ {table_name}): {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€è¡Œ
        except Exception as e:
            if conn:
                conn.rollback()
            print(f"âŒ æ’å…¥æ•°æ®å¤±è´¥ (è¡¨ {table_name}): {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€è¡Œ
        finally:
            if conn:
                conn.close()
    
    def convert_value_for_column(self, value, column_type: str, is_nullable: bool):
        """æ ¹æ®åˆ—ç±»å‹è½¬æ¢å€¼"""
        if value is None:
            if is_nullable:
                return None
            else:
                # æ ¹æ®åˆ—ç±»å‹æä¾›é»˜è®¤å€¼
                if 'int' in column_type:
                    return 0
                elif 'float' in column_type or 'numeric' in column_type:
                    return 0.0
                elif 'bool' in column_type:
                    return False
                else:
                    return ''
        
        # å¤„ç†å­—ç¬¦ä¸²ç±»å‹
        if isinstance(value, str):
            if value.lower() == 'null':
                return None if is_nullable else ''
            elif value.lower() == 'true':
                return True
            elif value.lower() == 'false':
                return False
        
        # æ ¹æ®åˆ—ç±»å‹è¿›è¡Œè½¬æ¢
        try:
            if 'int' in column_type:
                return int(value) if value is not None else 0
            elif 'float' in column_type or 'numeric' in column_type:
                return float(value) if value is not None else 0.0
            elif 'bool' in column_type:
                if isinstance(value, bool):
                    return value
                elif isinstance(value, str):
                    return value.lower() in ('true', '1', 'yes', 'on')
                else:
                    return bool(value)
            else:
                # æ–‡æœ¬ç±»å‹ï¼Œç¡®ä¿æ˜¯å­—ç¬¦ä¸²
                return str(value) if value is not None else ''
        except (ValueError, TypeError) as e:
            print(f"âš ï¸ æ•°æ®ç±»å‹è½¬æ¢å¤±è´¥: {value} -> {column_type}: {e}")
            # è¿”å›åŸå§‹å€¼
            return value
    
    def has_primary_key(self, table_name: str) -> bool:
        """æ£€æŸ¥è¡¨æ˜¯å¦æœ‰ä¸»é”®"""
        try:
            conn = psycopg2.connect(
                host=self.db_config.host,
                port=self.db_config.port,
                database=self.db_config.database,
                user=self.db_config.username,
                password=self.db_config.password
            )
            cursor = conn.cursor()
            
            cursor.execute(f"""
                SELECT COUNT(*) 
                FROM information_schema.table_constraints 
                WHERE table_name = %s 
                AND constraint_type = 'PRIMARY KEY'
            """, (table_name,))
            
            result = cursor.fetchone()
            has_pk = result[0] > 0 if result else False
            
            conn.close()
            return has_pk
            
        except Exception as e:
            print(f"æ£€æŸ¥ä¸»é”®å¤±è´¥: {e}")
            return False
    
    def filter_cleanup_commands(self, sql_content: str) -> str:
        """è¿‡æ»¤æ‰æ¸…ç†å‘½ä»¤ï¼Œåªä¿ç•™æ•°æ®æ’å…¥éƒ¨åˆ†"""
        lines = sql_content.split('\n')
        filtered_lines = []
        in_data_section = False
        
        for line in lines:
            line_upper = line.upper().strip()
            
            # è·³è¿‡æ•°æ®åº“çº§åˆ«çš„æ“ä½œ
            if any(keyword in line_upper for keyword in [
                'DROP DATABASE', 'CREATE DATABASE', 'DROP SCHEMA', 'CREATE SCHEMA'
            ]):
                continue
            
            # è·³è¿‡è¡¨ç»“æ„åˆ é™¤æ“ä½œ
            if any(keyword in line_upper for keyword in [
                'DROP TABLE IF EXISTS', 'DROP TABLE', 'DROP SEQUENCE', 'DROP INDEX',
                'DROP VIEW', 'DROP FUNCTION', 'DROP TRIGGER', 'DROP RULE'
            ]):
                continue
            
            # è·³è¿‡COMMENT ONç­‰å…ƒæ•°æ®å‘½ä»¤
            if line_upper.startswith('COMMENT ON'):
                continue
            
            # è·³è¿‡SETè¯­å¥ï¼ˆç¯å¢ƒè®¾ç½®ï¼‰
            if line_upper.startswith('SET ') and not in_data_section:
                continue
            
            # è·³è¿‡SELECT pg_catalog.set_configç­‰ç³»ç»Ÿè°ƒç”¨
            if 'pg_catalog.set_config' in line_upper and not in_data_section:
                continue
            
            # æ£€æµ‹æ•°æ®éƒ¨åˆ†çš„å¼€å§‹
            if 'COPY ' in line_upper or line_upper.startswith('INSERT INTO'):
                in_data_section = True
            
            # ä¿ç•™CREATE TABLEè¯­å¥ï¼ˆè¡¨ç»“æ„ï¼‰
            if line_upper.startswith('CREATE TABLE'):
                filtered_lines.append(line)
                continue
            
            # ä¿ç•™æ‰€æœ‰æ•°æ®æ“ä½œè¯­å¥
            if in_data_section or any(keyword in line_upper for keyword in [
                'INSERT INTO', 'COPY ', 'SELECT ', 'VALUES'
            ]):
                filtered_lines.append(line)
                continue
            
            # ä¿ç•™å…¶ä»–å¯èƒ½æœ‰ç”¨çš„è¯­å¥
            if not line_upper.startswith('--') and line.strip():  # ä¿ç•™éæ³¨é‡Šè¡Œ
                filtered_lines.append(line)
        
        return '\n'.join(filtered_lines)
    
    async def execute_filtered_restore(self, sql_content: str):
        """æ‰§è¡Œè¿‡æ»¤åçš„SQLæ¢å¤"""
        # æ„å»ºpsqlå‘½ä»¤
        cmd = [
            'psql',
            f'--host={self.db_config.host}',
            f'--port={self.db_config.port}',
            f'--username={self.db_config.username}',
            f'--dbname={self.db_config.database}',
            '--quiet'
        ]
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env['PGPASSWORD'] = self.db_config.password
        
        # æ‰§è¡ŒSQLæ¢å¤
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            env=env
        )
        
        stdout, stderr = await process.communicate(input=sql_content.encode('utf-8'))
        
        if process.returncode != 0:
            error_msg = stderr.decode('utf-8', errors='replace').strip()
            raise Exception(f"å¢é‡æ¢å¤å¤±è´¥: {error_msg}")
    
    async def clear_database(self):
        """æ¸…ç©ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨"""
        try:
            conn = psycopg2.connect(
                host=self.db_config.host,
                port=self.db_config.port,
                database=self.db_config.database,
                user=self.db_config.username,
                password=self.db_config.password
            )
            cursor = conn.cursor()
            
            # è·å–æ‰€æœ‰è¡¨å
            cursor.execute("""
                SELECT tablename FROM pg_tables 
                WHERE schemaname = 'public'
            """)
            tables = cursor.fetchall()
            
            # åˆ é™¤æ‰€æœ‰è¡¨
            for table in tables:
                table_name = table[0]
                cursor.execute(f"DROP TABLE IF EXISTS {table_name} CASCADE")
            
            conn.commit()
            conn.close()
            print(f"å·²æ¸…ç©ºæ•°æ®åº“ä¸­çš„ {len(tables)} ä¸ªè¡¨")
            
        except Exception as e:
            raise Exception(f"æ¸…ç©ºæ•°æ®åº“å¤±è´¥: {str(e)}")
    
    async def clear_table_data_only(self):
        """åªæ¸…ç©ºè¡¨æ•°æ®ï¼Œä¿ç•™è¡¨ç»“æ„"""
        try:
            conn = psycopg2.connect(
                host=self.db_config.host,
                port=self.db_config.port,
                database=self.db_config.database,
                user=self.db_config.username,
                password=self.db_config.password
            )
            cursor = conn.cursor()
            
            # è·å–æ‰€æœ‰è¡¨å
            cursor.execute("""
                SELECT tablename FROM pg_tables 
                WHERE schemaname = 'public'
            """)
            tables = cursor.fetchall()
            
            # åªæ¸…ç©ºè¡¨æ•°æ®ï¼Œä¸åˆ é™¤è¡¨ç»“æ„
            for table in tables:
                table_name = table[0]
                cursor.execute(f"TRUNCATE TABLE {table_name} CASCADE")
            
            conn.commit()
            conn.close()
            print(f"å·²æ¸…ç©º {len(tables)} ä¸ªè¡¨çš„æ•°æ®ï¼Œä¿ç•™è¡¨ç»“æ„")
            
        except Exception as e:
            raise Exception(f"æ¸…ç©ºè¡¨æ•°æ®å¤±è´¥: {str(e)}")
    
    def filter_for_incremental_restore(self, sql_content: str) -> str:
        """ä¸ºå¢é‡æ¢å¤è¿‡æ»¤SQLï¼Œåªä¿ç•™æ•°æ®æ’å…¥éƒ¨åˆ†"""
        lines = sql_content.split('\n')
        filtered_lines = []
        in_data_section = False
        
        for line in lines:
            line_upper = line.upper().strip()
            
            # è·³è¿‡æ•°æ®åº“çº§åˆ«çš„æ“ä½œ
            if any(keyword in line_upper for keyword in [
                'DROP DATABASE', 'CREATE DATABASE', 'DROP SCHEMA', 'CREATE SCHEMA'
            ]):
                continue
            
            # è·³è¿‡è¡¨ç»“æ„æ“ä½œï¼ˆä¿ç•™ç°æœ‰è¡¨ç»“æ„ï¼‰
            if any(keyword in line_upper for keyword in [
                'DROP TABLE IF EXISTS', 'DROP TABLE', 'CREATE TABLE', 'DROP SEQUENCE', 
                'DROP INDEX', 'DROP VIEW', 'DROP FUNCTION', 'DROP TRIGGER', 'DROP RULE',
                'ALTER TABLE', 'CREATE INDEX', 'CREATE SEQUENCE'
            ]):
                continue
            
            # è·³è¿‡COMMENT ONç­‰å…ƒæ•°æ®å‘½ä»¤
            if line_upper.startswith('COMMENT ON'):
                continue
            
            # è·³è¿‡SETè¯­å¥ï¼ˆç¯å¢ƒè®¾ç½®ï¼‰
            if line_upper.startswith('SET ') and not in_data_section:
                continue
            
            # è·³è¿‡SELECT pg_catalog.set_configç­‰ç³»ç»Ÿè°ƒç”¨
            if 'pg_catalog.set_config' in line_upper and not in_data_section:
                continue
            
            # æ£€æµ‹æ•°æ®éƒ¨åˆ†çš„å¼€å§‹
            if 'COPY ' in line_upper or line_upper.startswith('INSERT INTO'):
                in_data_section = True
            
            # ä¿ç•™æ‰€æœ‰æ•°æ®æ“ä½œè¯­å¥
            if in_data_section or any(keyword in line_upper for keyword in [
                'INSERT INTO', 'COPY ', 'SELECT ', 'VALUES'
            ]):
                filtered_lines.append(line)
                continue
            
            # ä¿ç•™å…¶ä»–å¯èƒ½æœ‰ç”¨çš„è¯­å¥ï¼ˆä½†ä¸åŒ…æ‹¬è¡¨ç»“æ„ï¼‰
            if not line_upper.startswith('--') and line.strip() and not any(keyword in line_upper for keyword in [
                'CREATE', 'DROP', 'ALTER'
            ]):
                filtered_lines.append(line)
        
        return '\n'.join(filtered_lines)
    
    async def execute_restore(self, backup_file: str, compressed: bool):
        """æ‰§è¡Œæ¢å¤å‘½ä»¤"""
        # æ„å»ºpsqlå‘½ä»¤
        cmd = [
            'psql',
            f'--host={self.db_config.host}',
            f'--port={self.db_config.port}',
            f'--username={self.db_config.username}',
            f'--dbname={self.db_config.database}',
            '--quiet'
        ]
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env['PGPASSWORD'] = self.db_config.password
        
        # è¯»å–SQLå†…å®¹
        if compressed:
            # ä»å‹ç¼©æ–‡ä»¶è¯»å–å†…å®¹
            with gzip.open(backup_file, 'rt', encoding='utf-8') as f:
                sql_content = f.read()
        else:
            # ä»æ™®é€šæ–‡ä»¶è¯»å–å†…å®¹
            with open(backup_file, 'r', encoding='utf-8') as f:
                sql_content = f.read()
        
        # æ‰§è¡ŒSQLæ¢å¤
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            env=env
        )
        
        stdout, stderr = await process.communicate(input=sql_content.encode('utf-8'))
        
        if process.returncode != 0:
            error_msg = stderr.decode('utf-8', errors='replace').strip()
            raise Exception(f"æ¢å¤å¤±è´¥: {error_msg}")
    
    def get_latest_backup(self) -> Optional[BackupInfo]:
        """è·å–æœ€æ–°çš„å¤‡ä»½"""
        backups = self.backup_manager.get_backup_list()
        completed_backups = [b for b in backups if b.status == "completed"]
        return completed_backups[0] if completed_backups else None
    
    async def test_connection(self) -> bool:
        """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
        try:
            conn = psycopg2.connect(
                host=self.db_config.host,
                port=self.db_config.port,
                database=self.db_config.database,
                user=self.db_config.username,
                password=self.db_config.password
            )
            conn.close()
            return True
        except Exception as e:
            print(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    
    async def get_database_info(self) -> dict:
        """è·å–æ•°æ®åº“ä¿¡æ¯"""
        try:
            conn = psycopg2.connect(
                host=self.db_config.host,
                port=self.db_config.port,
                database=self.db_config.database,
                user=self.db_config.username,
                password=self.db_config.password
            )
            cursor = conn.cursor()
            
            # è·å–æ•°æ®åº“å¤§å°
            cursor.execute(f"SELECT pg_size_pretty(pg_database_size('{self.db_config.database}'))")
            db_size_result = cursor.fetchone()
            db_size = db_size_result[0] if db_size_result else "Unknown"
            
            # è·å–è¡¨æ•°é‡
            cursor.execute("""
                SELECT count(*) FROM information_schema.tables 
                WHERE table_schema = 'public'
            """)
            table_count_result = cursor.fetchone()
            table_count = table_count_result[0] if table_count_result else 0
            
            # è·å–è¡¨åå’Œè¡¨ç»“æ„ä¿¡æ¯
            cursor.execute("""
                SELECT 
                    table_name,
                    column_name,
                    data_type,
                    is_nullable,
                    column_default
                FROM information_schema.columns
                WHERE table_schema = 'public'
                ORDER BY table_name, ordinal_position
            """)
            columns_result = cursor.fetchall()
            
            # ç»„ç»‡è¡¨ç»“æ„æ•°æ®
            tables = {}
            for row in columns_result:
                table_name, column_name, data_type, is_nullable, column_default = row
                if table_name not in tables:
                    tables[table_name] = []
                tables[table_name].append({
                    "column_name": column_name,
                    "data_type": data_type,
                    "is_nullable": is_nullable,
                    "column_default": column_default
                })
            
            conn.close()
            
            return {
                "database_size": db_size,
                "table_count": table_count,
                "tables": tables
            }
            
        except Exception as e:
            return {"error": str(e)} 